{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54056b41",
   "metadata": {},
   "source": [
    "### Basic Workflow Creation\n",
    "\n",
    "\n",
    "pip install llama-index llama-index-vector-stores-chroma llama-index-utils-workflow llama-index-llms-huggingface-api pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b6d0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step\n",
    "\n",
    "class MyWorkFlow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
    "        return StopEvent(result=\"Hello, world!\")\n",
    "\n",
    "\n",
    "w = MyWorkFlow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a616f5",
   "metadata": {},
   "source": [
    "### Connecting Multiple Steps\n",
    "\n",
    "We can also create multi-step workflows. Here we pass the event information between steps. Note that we can use type hinting to specify the event type and the flow of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047c9cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finished processing: Step 1 complete'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
    "        # process initial data\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "    \n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "    \n",
    "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c4fed",
   "metadata": {},
   "source": [
    "### Loops and Branches\n",
    "We can also use type hinting to create branches and loops. Note tha we can use the `|` operator to specify that the step can return multiple types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5723de87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bas thing happened\n",
      "Bas thing happened\n",
      "Bas thing happened\n",
      "Bas thing happened\n",
      "Good thing happened.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished processing: First step complete'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "import random\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "class LoopEvent(Event):\n",
    "    loop_output: str\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Bas thing happened\")\n",
    "            return LoopEvent(loop_output=\"Back to step one.\")\n",
    "        else:\n",
    "            print(\"Good thing happened.\")\n",
    "            return ProcessingEvent(intermediate_result=\"First step complete\")\n",
    "        \n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "    \n",
    "w = MultiStepWorkflow(verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3ab909",
   "metadata": {},
   "source": [
    "### Drawing workflows\n",
    "\n",
    "We can also draw workflows using the `draw_all_possible_flows` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f081eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workflow_all_flows.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d32d14",
   "metadata": {},
   "source": [
    "### State Management\n",
    "\n",
    "Instead of passing the event information between steps, we can use the `Context` type hint to pass information between steps. This might be useful for long running workflows,\n",
    "Where you want to store information between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bc1f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the capital of France?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished processing: Step 1 complete'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event, Context\n",
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent, ctx: Context) -> ProcessingEvent:\n",
    "        # processing initial data\n",
    "        await ctx.store.set(\"query\", \"What is the capital of France?\")\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "    \n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent, ctx: Context) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        query = await ctx.store.get(\"query\")\n",
    "        print(f\"Query: {query}\")\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "    \n",
    "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812b07f",
   "metadata": {},
   "source": [
    "### Multi-Agent Workflows\n",
    "\n",
    "We can also create multi-agent workflows. Here we define two agents, one that multiplies two integers and one that adds two integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a71a8f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='5 + 3 = 8')]), structured_response=None, current_agent_name='add_agent', raw={'model': 'kimi-k2:1t-cloud', 'created_at': '2026-01-11T13:09:27.480663638Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1162937585, 'load_duration': None, 'prompt_eval_count': 822, 'prompt_eval_duration': None, 'eval_count': 30, 'eval_duration': None, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_name=None, tool_calls=None), 'logprobs': None, 'usage': {'prompt_tokens': 822, 'completion_tokens': 30, 'total_tokens': 852}}, tool_calls=[ToolCallResult(tool_name='handoff', tool_kwargs={'to_agent': 'add_agent', 'reason': 'The user wants to add 5 and 3, but I only have a multiply tool. The add_agent is better suited to handle this addition request.'}, tool_id='3e519276-7156-4627-a850-507a1efc9a96', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text='Agent add_agent is now handling the request due to the following reason: The user wants to add 5 and 3, but I only have a multiply tool. The add_agent is better suited to handle this addition request..\\nPlease continue with the current request.')], tool_name='handoff', raw_input={'args': (), 'kwargs': {'to_agent': 'add_agent', 'reason': 'The user wants to add 5 and 3, but I only have a multiply tool. The add_agent is better suited to handle this addition request.'}}, raw_output='Agent add_agent is now handling the request due to the following reason: The user wants to add 5 and 3, but I only have a multiply tool. The add_agent is better suited to handle this addition request..\\nPlease continue with the current request.', is_error=False), return_direct=True)], retry_messages=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a : int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm = Ollama(model=\"kimi-k2:1t-cloud\", base_url='http://127.0.0.1:11434')\n",
    "\n",
    "# We can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description\n",
    "multiply_agent = ReActAgent(\n",
    "    name=\"multiply_agent\",\n",
    "    description=\"Is able to multiply two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\",\n",
    "    tools=[multiply],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "addition_agent = ReActAgent(\n",
    "    name = \"add_agent\",\n",
    "    description = \"Is able to add two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to add numbers.\",\n",
    "    tools = [add],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# create the workflow\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent=\"multiply_agent\"\n",
    ")\n",
    "\n",
    "# Run the system\n",
    "response = await workflow.run(user_msg=\"Can you add 5 and 3?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96abe2f",
   "metadata": {},
   "source": [
    "Agent tools can also modify the workflow state we mentioned earlier. Before starting the workflow, we can provide an initial state dict that will be available to all agents. The state is stored in the state key of the workflow context. It will be injected into the state_prompt which augments each new user message.\n",
    "\n",
    "Letâ€™s inject a counter to count function calls by modifying the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19bbb842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# Define some tools\n",
    "async def add(ctx: Context, a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    # update our count\n",
    "    cur_state = await ctx.store.get(\"state\")\n",
    "    cur_state[\"num_fn_calls\"] += 1\n",
    "    await ctx.store.set(\"state\", cur_state)\n",
    "\n",
    "    return a + b\n",
    "\n",
    "async def multiply(ctx: Context, a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    # update our count\n",
    "    cur_state = await ctx.store.get(\"state\")\n",
    "    cur_state[\"num_fn_calls\"] += 1\n",
    "    await ctx.store.set(\"state\", cur_state)\n",
    "\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# We can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description\n",
    "multiply_agent = ReActAgent(\n",
    "    name=\"multiply_agent\",\n",
    "    description=\"Is able to multiply two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\",\n",
    "    tools=[multiply],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "addition_agent = ReActAgent(\n",
    "    name = \"add_agent\",\n",
    "    description = \"Is able to add two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to add numbers.\",\n",
    "    tools = [add],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# defining workflow\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent=\"multiply_agent\",\n",
    "    initial_state={\"num_fn_calls\": 0},\n",
    "    state_prompt=\"Current state: {state}. User message: {msg}\",\n",
    ")\n",
    "\n",
    "ctx = Context(workflow)\n",
    "response = await workflow.run(user_msg=\"Can you add 5 and 3?\", ctx=ctx)\n",
    "\n",
    "# pull out and inspect the state\n",
    "state = await ctx.store.get('state')\n",
    "print(state[\"num_fn_calls\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
