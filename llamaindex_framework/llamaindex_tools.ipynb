{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691583e1",
   "metadata": {},
   "source": [
    "#### Using Tools in LlamaIndex\n",
    "\n",
    "Defining a clear set of Tools is crucial to performance. As we discussed in unit 1, clear tool interfaces are easier for LLMs to use. Much like a software API interface for human engineers, they can get more out of the tool if it’s easy to understand how it works.\n",
    "\n",
    "There are four main types of tools in LlamaIndex:\n",
    "\n",
    "1. FunctionTool: Convert any Python function into a tool that an agent can use. It automatically figures out how the function works.\n",
    "2. QueryEngineTool: A tool that lets agents use query engines. Since agents are built on query engines, they can also use other agents as tools.\n",
    "Toolspecs: Sets of tools created by the community, which often include tools for specific services like Gmail.\n",
    "3. Utility Tools: Special tools that help handle large amounts of data from other tools.\n",
    "\n",
    "We will go over each of them in more detail below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e1cf42",
   "metadata": {},
   "source": [
    "### Creating a FunctionTool\n",
    "\n",
    "A FunctionTool provides a simple way to wrap any Python function and make it available to an agent. You can pass either a synchronous or asynchronous function to the tool, along with optional name and description parameters. The name and description are particularly important as they help the agent understand when and how to use the tool effectively. Let’s look at how to create a FunctionTool below and then call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6190cf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for New York\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(blocks=[TextBlock(block_type='text', text='The weather in New York is sunny')], tool_name='my_weather_tool', raw_input={'args': ('New York',), 'kwargs': {}}, raw_output='The weather in New York is sunny', is_error=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny\"\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"my_weather_tool\",\n",
    "    description=\"Useful for getting the weather for a given location.\"\n",
    ")\n",
    "\n",
    "tool.call(\"New York\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2358a47",
   "metadata": {},
   "source": [
    "### Creating a QueryEngineTool\n",
    "\n",
    "The QueryEngine we defined in the previous unit can be easily transformed into a tool using the QueryEngineTool class. Let’s see how to create a QueryEngineTool from a QueryEngine in the example below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f08fe",
   "metadata": {},
   "source": [
    "I will use Ollama for my purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58d8cdf",
   "metadata": {},
   "source": [
    "Use pip install llama-index-vector-stores-chroma to install chroma vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b5dabd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
    "\n",
    "llm = Ollama(model=\"kimi-k2:1t-cloud\", base_url='http://127.0.0.1:11434')\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "tool = QueryEngineTool.from_defaults(query_engine, name=\"some useful name\", description=\"some useful description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc073b3b",
   "metadata": {},
   "source": [
    "### Creating Toolspecs\n",
    "Think of ToolSpecs as collections of tools that work together harmoniously - like a well-organized professional toolkit. Just as a mechanic’s toolkit contains complementary tools that work together for vehicle repairs, a ToolSpec combines related tools for specific purposes. For example, an accounting agent’s ToolSpec might elegantly integrate spreadsheet capabilities, email functionality, and calculation tools to handle financial tasks with precision and efficiency.\n",
    "\n",
    "Install the Google Toolspec\n",
    "As introduced in the section on the LlamaHub, we can install the Google toolspec with the following command:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ae253b",
   "metadata": {},
   "source": [
    "```python\n",
    "pip install llama-index-tools-google\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8557c95e",
   "metadata": {},
   "source": [
    "And now we can load the toolspec and convert it to a list of tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a12aae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.google import GmailToolSpec\n",
    "\n",
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568531aa",
   "metadata": {},
   "source": [
    "To get a more detailed view of the tools, we can take a look at the metadata of each tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d650588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('load_data',\n",
       "  \"load_data() -> List[llama_index.core.schema.Document]\\nLoad emails from the user's account.\"),\n",
       " ('search_messages',\n",
       "  \"search_messages(query: str, max_results: Optional[int] = None)\\n\\nSearches email messages given a query string and the maximum number\\nof results requested by the user\\n   Returns: List of relevant message objects up to the maximum number of results.\\n\\nArgs:\\n    query (str): The user's query\\n    max_results (Optional[int]): The maximum number of search results\\n    to return.\"),\n",
       " ('create_draft',\n",
       "  \"create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\\n\\nCreate and insert a draft email.\\n   Print the returned draft's message and id.\\n   Returns: Draft object, including draft id and message meta data.\\n\\nArgs:\\n    to (Optional[str]): The email addresses to send the message to\\n    subject (Optional[str]): The subject for the event\\n    message (Optional[str]): The message for the event\"),\n",
       " ('update_draft',\n",
       "  \"update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\\n\\nUpdate a draft email.\\n   Print the returned draft's message and id.\\n   This function is required to be passed a draft_id that is obtained when creating messages\\n   Returns: Draft object, including draft id and message meta data.\\n\\nArgs:\\n    to (Optional[str]): The email addresses to send the message to\\n    subject (Optional[str]): The subject for the event\\n    message (Optional[str]): The message for the event\\n    draft_id (str): the id of the draft to be updated\"),\n",
       " ('get_draft',\n",
       "  \"get_draft(draft_id: str = None) -> str\\n\\nGet a draft email.\\n   Print the returned draft's message and id.\\n   Returns: Draft object, including draft id and message meta data.\\n\\nArgs:\\n    draft_id (str): the id of the draft to be updated\"),\n",
       " ('send_draft',\n",
       "  \"send_draft(draft_id: str = None) -> str\\n\\nSends a draft email.\\n   Print the returned draft's message and id.\\n   Returns: Draft object, including draft id and message meta data.\\n\\nArgs:\\n    draft_id (str): the id of the draft to be updated\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea16458",
   "metadata": {},
   "source": [
    "### Model Context Protocol (MCP) in LlamaIndex\n",
    "LlamaIndex also allows using MCP tools through a ToolSpec on the LlamaHub. You can simply run an MCP server and start using it through the following implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb980fbb",
   "metadata": {},
   "source": [
    "Installation command:\n",
    "```python\n",
    "pip install llama-index-tools-mcp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e34037e",
   "metadata": {},
   "source": [
    "Below code is only for representation, it doesn't works as of now, I will update this code block after completing MCP course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3db708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "\n",
    "# We consider there is a mcp server running on 127.0.0.1:8000, or you can use the mcp client to connect to your own mcp server.\n",
    "mcp_client = BasicMCPClient(\"http://127.0.0.1:8000/sse\")\n",
    "mcp_tool = McpToolSpec(client=mcp_client)\n",
    "\n",
    "# get the agent\n",
    "agent = await get_agent(mcp_tool)\n",
    "\n",
    "# create the agent context\n",
    "agent_context = Context(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
