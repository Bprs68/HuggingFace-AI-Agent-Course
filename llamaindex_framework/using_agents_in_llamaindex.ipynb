{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08ba65f",
   "metadata": {},
   "source": [
    "### Using Agents in LlamaIndex\n",
    "\n",
    "Remember Alfred, our helpful butler agent from earlier? Well, he’s about to get an upgrade! Now that we understand the tools available in LlamaIndex, we can give Alfred new capabilities to serve us better.\n",
    "\n",
    "But before we continue, let’s remind ourselves what makes an agent like Alfred tick. Back in Unit 1, we learned that:\n",
    "\n",
    "An Agent is a system that leverages an AI model to interact with its environment to achieve a user-defined objective. It combines reasoning, planning, and action execution (often via external tools) to fulfil tasks.\n",
    "\n",
    "LlamaIndex supports three main types of reasoning agents:\n",
    "\n",
    "1. Function Calling Agents - These work with AI models that can call specific functions.\n",
    "2. ReAct Agents - These can work with any AI that does chat or text endpoint and deal with complex reasoning tasks.\n",
    "3. Advanced Custom Agents - These use more complex methods to deal with more complex tasks and workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca66b43",
   "metadata": {},
   "source": [
    "### Initialising Agents\n",
    "To create an agent, we start by providing it with a set of functions/tools that define its capabilities. Let’s look at how to create an agent with some basic tools. As of this writing, the agent will automatically use the function calling API (if available), or a standard ReAct agent loop.\n",
    "\n",
    "LLMs that support a tools/functions API are relatively new, but they provide a powerful way to call tools by avoiding specific prompting and allowing the LLM to create tool calls based on provided schemas.\n",
    "\n",
    "ReAct agents are also good at complex reasoning tasks and can work with any LLM that has chat or text completion capabilities. They are more verbose, and show the reasoning behind certain actions that they take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa3c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# define sample Tool -- type annotations, function names, and docstrings, are all included in parsed schemas!\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies two integers and returns the resulting integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# initialize llm\n",
    "llm = Ollama(model=\"kimi-k2:1t-cloud\", base_url='http://127.0.0.1:11434')\n",
    "\n",
    "# initialize agent\n",
    "agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [FunctionTool.from_defaults(multiply)],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec98ba",
   "metadata": {},
   "source": [
    "Agents are stateless by default, however, they can remember past interactions using a Context object. This might be useful if you want to use an agent that needs to remember previous interactions, like a chatbot that maintains context across multiple messages or a task manager that needs to track progress over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6cd76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stateless\n",
    "response = await agent.run(\"What is 2 times 2?\")\n",
    "\n",
    "# remembering state\n",
    "from llama_index.core.workflow import Context\n",
    "\n",
    "ctx = Context(agent)\n",
    "\n",
    "response = await agent.run(\"My name is Bob.\", ctx=ctx)\n",
    "response = await agent.run(\"What was my name again?\", ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b6beefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You said your name is Bob.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f65a2c",
   "metadata": {},
   "source": [
    "#### Creating RAG Agents with QueryEngineTools\n",
    "\n",
    "Agentic RAG is a powerful way to use agents to answer questions about your data. We can pass various tools to Alfred to help him answer questions. However, instead of answering the question on top of documents automatically, Alfred can decide to use any other tool or flow to answer the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927098b8",
   "metadata": {},
   "source": [
    "It is easy to wrap QueryEngine as a tool for an agent. When doing so, we need to define a name and description. The LLM will use this information to correctly use the tool. Let’s see how to load in a QueryEngineTool using the QueryEngine we created in the component section.\n",
    "\n",
    "```python\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "query_engine = index.as_query_engine(llm=llm, similarity_top_k=3) # as shown in the Components in LlamaIndex section\n",
    "\n",
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"name\",\n",
    "    description=\"a specific description\",\n",
    "    return_direct=False,\n",
    ")\n",
    "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
    "    [query_engine_tool],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions. \"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0346f",
   "metadata": {},
   "source": [
    "#### Creating Multi-agent systems\n",
    "The AgentWorkflow class also directly supports multi-agent systems. By giving each agent a name and description, the system maintains a single active speaker, with each agent having the ability to hand off to another agent.\n",
    "\n",
    "By narrowing the scope of each agent, we can help increase their general accuracy when responding to user messages.\n",
    "\n",
    "Agents in LlamaIndex can also directly be used as tools for other agents, for more complex and custom scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "115ba348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentWorkflow,\n",
    "    FunctionAgent,\n",
    "    ReActAgent\n",
    ")\n",
    "\n",
    "#Define some tools\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract two numbers.\"\"\"\n",
    "    return a - b\n",
    "\n",
    "# Create agent configs\n",
    "# Note: We can use FunctionAgent ot ReAct Agent here.\n",
    "# FunctionAgent works for LLMs with a function calling API.\n",
    "# ReActAgent works for any LLM.\n",
    "\n",
    "calculator_agent = ReActAgent(\n",
    "    name=\"calculator\",\n",
    "    description=\"Performs basic arithmetic operations\",\n",
    "    system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\",\n",
    "    tools=[add, subtract],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# query_agent = ReActAgent(\n",
    "#     name=\"info_lookup\",\n",
    "#     description=\"Looks up information about XYZ\",\n",
    "#     system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\",\n",
    "#     tools=[query_engine_tool],\n",
    "#     llm=llm\n",
    "# )\n",
    "\n",
    "# Create and run the workflow\n",
    "agent = AgentWorkflow(\n",
    "    agents=[calculator_agent], root_agent=\"calculator\"\n",
    ")\n",
    "\n",
    "#run the system\n",
    "response = await agent.run(user_msg=\"Can you add 5 and 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ade265bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f407c1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
